{"cells":[{"cell_type":"markdown","metadata":{"id":"96d9b0ee"},"source":["# 43008: Reinforcement Learning\n","\n","## Week 6: Monte Carlo Methods - Control:\n","* MC Algorithm for finding optimal policy - Model free approach\n","\n","### What you will learn?\n","* GLIE-MC Algorithm for finding optimal policy for Gym based environments"]},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"CNchkeOL4MEy"}},{"cell_type":"code","source":["!pip install gymnasium"],"metadata":{"id":"Savbxz2f4Yz8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gymnasium as gym\n","import numpy as np\n","from mpl_toolkits.mplot3d import Axes3D\n","import matplotlib.pyplot as plt\n","import sys"],"metadata":{"id":"9Iq3RdInMXEY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Helper function for visualisation"],"metadata":{"id":"w55IbhMp0G0L"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def plot_blackjack_values(Q):\n","    \"\"\"\n","    Given a Q-table, this function plots the state-value function for Blackjack,\n","    aligning with the representation in Sutton and Barto's book.\n","\n","    Args:\n","    - Q (dict): The Q-table.\n","    \"\"\"\n","\n","    V = {}  # For storing the state-value function\n","\n","    # Populate the V dictionary with state-value pairs\n","    for k, v in Q.items():\n","        # Check if v is a dictionary (i.e., nested dictionary structure)\n","        if isinstance(v, dict):\n","            values = list(v.values())\n","        else:\n","            values = v\n","\n","        # If the values list has only one entry, pad it with zero to make it a two-element list\n","        if len(values) == 1:\n","            values.append(0.0)\n","\n","        # Compute the state-value based on a policy:\n","        # If player's current sum is greater than 18, the policy is 80% stick and 20% hit.\n","        # Otherwise, the policy is 20% stick and 80% hit.\n","        if k[0] > 18:\n","            V[k] = np.dot([0.8, 0.2], values)\n","        else:\n","            V[k] = np.dot([0.2, 0.8], values)\n","\n","    # Helper function to get Z values for plotting\n","    def get_Z(x, y, usable_ace):\n","        # Return the state value if the state exists in V, otherwise return 0\n","        if (x, y, usable_ace) in V:\n","            return V[(x, y, usable_ace)]\n","        else:\n","            return 0\n","\n","    # Helper function to generate the 3D surface plot for a given usable ace condition\n","    def get_figure(usable_ace, ax):\n","        # Define the range for player's sum and dealer's card\n","        x_range = np.arange(12, 22)  # Player's sum: 12 through 21\n","        y_range = np.arange(10, 0, -1)  # Dealer's card: 10 through Ace\n","\n","        # Create a meshgrid for plotting\n","        X, Y = np.meshgrid(x_range, y_range)\n","\n","        # Calculate Z values using the get_Z function\n","        Z = np.array([get_Z(x, y, usable_ace) for x, y in zip(np.ravel(X), np.ravel(Y))]).reshape(X.shape)\n","\n","        # Plot the surface with the provided colormap\n","        surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.cm.coolwarm, vmin=-1.0, vmax=1.0)\n","\n","        # Label the axes\n","        ax.set_xlabel(\"Player's Current Sum\")\n","        ax.set_ylabel(\"Dealer's Showing Card\")\n","        ax.set_yticks(np.arange(1, 11))\n","        ax.set_yticklabels(['10', '9', '8', '7', '6', '5', '4', '3', '2', 'A'])  # Reverse order for y-axis labels\n","        ax.set_zlabel('State Value')\n","\n","        # Set the viewing angle to give the desired orientation\n","        ax.view_init(ax.elev, -120)\n","\n","    # Create the 3D plot\n","    fig = plt.figure(figsize=(20, 20))\n","\n","    # Plot for the case with a usable ace\n","    ax = fig.add_subplot(211, projection='3d')\n","    ax.set_title('Usable Ace')\n","    get_figure(True, ax)\n","\n","    # Plot for the case without a usable ace\n","    ax = fig.add_subplot(212, projection='3d')\n","    ax.set_title('No Usable Ace')\n","    get_figure(False, ax)\n","\n","    # Display the plots\n","    plt.show()\n"],"metadata":{"id":"8c6tYsj2ZvnH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_blackjack_policy_from_Q(Q):\n","    \"\"\"\n","    Plot the Blackjack policy derived from Q values.\n","\n","    Args:\n","    - Q (dict): The Q-table, mapping states to action-value pairs.\n","    \"\"\"\n","\n","    # Define the range for dealer's card and player's sum\n","    x_range = np.arange(1, 11)  # Dealer's card: Ace through 10\n","    y_range = np.arange(12, 22)  # Player's sum: 12 through 21\n","\n","    # Create two 2D matrices to store policy data for usable and non-usable ace scenarios.\n","    policy_data_usable = np.zeros((len(y_range), len(x_range)))\n","    policy_data_non_usable = np.zeros((len(y_range), len(x_range)))\n","\n","    # Derive policy from Q values\n","    for state, actions in Q.items():\n","        player_sum, dealer_card, usable_ace = state\n","        # Get the action with the maximum Q value\n","        best_action = np.argmax(actions)\n","        if usable_ace:\n","            policy_data_usable[player_sum - 12, dealer_card - 1] = best_action\n","        else:\n","            policy_data_non_usable[player_sum - 12, dealer_card - 1] = best_action\n","\n","    fig, axes = plt.subplots(nrows=2, figsize=(10, 20))\n","\n","    # Plot the policy for states with a usable ace\n","    axes[0].matshow(policy_data_usable, cmap=plt.cm.coolwarm, extent=[0.5, 10.5, 21.5, 11.5])\n","    axes[0].set_title('Usable Ace')\n","    axes[0].invert_yaxis()  # Invert the y-axis\n","\n","    # Plot the policy for states without a usable ace\n","    axes[1].matshow(policy_data_non_usable, cmap=plt.cm.coolwarm, extent=[0.5, 10.5, 21.5, 11.5])\n","    axes[1].set_title('No Usable Ace')\n","    axes[1].invert_yaxis()  # Invert the y-axis\n","\n","    for ax in axes:\n","        ax.set_xticks(np.arange(1, 11))\n","        ax.set_yticks(np.arange(12, 22))\n","        ax.set_yticklabels(list(range(12, 22)))  # Corrected player sums labels\n","        ax.set_xlabel(\"Dealer's Showing Card\")\n","        ax.set_ylabel(\"Player's Current Sum\")\n","\n","    plt.tight_layout()\n","    plt.show()\n"],"metadata":{"id":"X-uUve7CaR_7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## GLIE MC-Control Algorithm\n","\n","\n","<img src='https://drive.google.com/uc?id=1kY0AdRSrCyZHRyWEo0wnjmLoAj9qCU9W' height=400>\n","\n","\n","##**Steps**:\n","\n","1.   Write a function for Implementing Epsilon-Greedy Policy\n","2.   Write a function for creating Episodes\n","3.   Write a function for Updating Q-Values\n","4.   Combine all the functions to complete the GLIE Control Algorithm\n","\n"],"metadata":{"id":"2oHL15XP04hV"}},{"cell_type":"markdown","source":["### 1. Implement Epsilon Greedy Policy"],"metadata":{"id":"OxPcr-LN04hV"}},{"cell_type":"code","source":["def epsilon_greedy_policy(state, Q, epsilon):\n","    \"\"\"Select an action using epsilon-greedy policy.\"\"\"\n","    if state not in Q:\n","        Q[state] = [0, 0]\n","\n","    # Randomly choose an action with probability epsilon (Exploration)\n","    if ## WRITE YOUR CODE HERE ##\n","        return ## WRITE YOUR CODE HERE ##\n","    # Choose the action with the highest Q-value for the current state (Exploitation)\n","    else:\n","        return ## WRITE YOUR CODE HERE ##"],"metadata":{"id":"Ma5sdHBL04hV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Implement Generate Episode function"],"metadata":{"id":"Nh_OQ32704hV"}},{"cell_type":"code","source":["# Generate Episode function\n","def generate_episode(env, Q, epsilon):\n","    \"\"\"Generate an episode following the epsilon-greedy policy.\"\"\"\n","\n","    # Intialize the Episode\n","    episode = ## WRITE YOUR CODE HERE ##\n","    # Reset environment to starting state\n","    state = ## WRITE YOUR CODE HERE ##\n","    done = ## WRITE YOUR CODE HERE ##\n","\n","    # Continue until the episode is done\n","    while not done:\n","        # Select an action based on epsilon-greedy policy\n","        action = ## WRITE YOUR CODE HERE ##\n","\n","        # Take the action and observe the next state and reward\n","        next_state, reward, done, _ = ## WRITE YOUR CODE HERE ##\n","\n","        # Append the state, action, and reward to the episode list\n","        episode.append ## WRITE YOUR CODE HERE ##\n","        state = ## WRITE YOUR CODE HERE ##\n","\n","    return episode"],"metadata":{"id":"zZgXYFqA04hW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Implement Q-Value Update"],"metadata":{"id":"EBVm-khF04hW"}},{"cell_type":"code","source":["def update_Q_values(episode, Q, N, gamma):\n","    \"\"\"Update Q-values using the generated episode.\"\"\"\n","    # Initialize the return (G_t) to zero\n","    G_t = ## WRITE YOUR CODE HERE ##\n","    # Iterate over the episode in reverse order\n","    for state, action, reward in reversed(episode):\n","        # Ensure the state is initialized in Q and N\n","        if state not in Q:\n","            Q[state] = [0, 0]\n","        if state not in N:\n","            N[state] = [0, 0]\n","\n","        # Calculate the cumulative discounted reward\n","        G_t = ## WRITE YOUR CODE HERE ##\n","        # Increment the count for the state-action pair\n","        N[state][action] += ## WRITE YOUR CODE HERE ##\n","\n","        # Calculate the step-size (alpha = 1/N(St,At))\n","        alpha = 1 / N[state][action]\n","\n","        # Incrementally update the Q-value using the return (G_t)\n","        Q[state][action] += ## WRITE YOUR CODE HERE ##"],"metadata":{"id":"pdnVmz-O04hW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4. Implement GLIE MC Control"],"metadata":{"id":"_W89ySeV04hW"}},{"cell_type":"code","source":["def glie_mc_control(env, episodes, gamma, epsilon, decay):\n","    \"\"\"Train the agent using GILE MC Control.\"\"\"\n","    # Initialize Q-values with zeros for each state-action pair\n","    Q = ## WRITE YOUR CODE HERE ##\n","    # Initialize N (counts) with zeros for each state-action pair\n","    N = ## WRITE YOUR CODE HERE ##\n","    # Iterate over the specified number of episodes\n","    for k in range(1, episodes + 1):\n","        # Generate an episode\n","        episode = ## WRITE YOUR CODE HERE ##\n","        print(f\"Episode {k}/{episodes}, Current epsilon: {epsilon}\")\n","        sys.stdout.flush()\n","        # Update Q-values using the episode\n","        ## WRITE YOUR CODE HERE ##\n","        # Decay epsilon to reduce exploration over time\n","        epsilon *= ## WRITE YOUR CODE HERE ##\n","\n","    # After all episodes are processed, derive the policy from the Q-values.\n","    policy = {}\n","    for state, actions in Q.items():\n","        policy[state] = ## WRITE YOUR CODE HERE ##\n","    print(f\"Derived policy for {len(policy)} states.\")  # Debug: Print the number of states for which policy is derived\n","    return Q, policy"],"metadata":{"id":"ICqPtWTc04hc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5. Solve the BlackJack environment using the GLIE MC Control Algorithm"],"metadata":{"id":"mVZimNMA04hc"}},{"cell_type":"code","source":["# Create the Blackjack environment\n","env = ## WRITE YOUR CODE HERE ##\n","\n","# Set hyperparameters\n","gamma = 0.9  # Discount factor\n","epsilon = 1.0  # Initial exploration factor\n","decay = 0.9995  # Decay rate for epsilon\n","episodes = 50000  # Number of training episodes\n","\n","# Train the agent\n","Q, policy = ## WRITE YOUR CODE HERE ##\n","\n","# Close the environment\n","env.close()\n"],"metadata":{"id":"SwdIjuGX04hc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Check the results: Optimal Policy and Q-Values"],"metadata":{"id":"SxzR8VBq04hc"}},{"cell_type":"code","source":["print(\"\\nOptimal Policy:\", policy)\n","print(\"\\nOptimal Q-values:\\n\", Q)"],"metadata":{"id":"TUdqmkhb04hc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Plot the graph of optimal Q Values"],"metadata":{"id":"IbZTqumP04hc"}},{"cell_type":"code","source":["plot_blackjack_values(Q)"],"metadata":{"id":"oWikoUgs04hd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Plot the graph of optimal Policy"],"metadata":{"id":"PBMiGn5D04hd"}},{"cell_type":"code","source":["plot_blackjack_policy_from_Q(Q)"],"metadata":{"id":"2jhj9MVl04hd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DPG0ijjW04hd"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["w55IbhMp0G0L"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}