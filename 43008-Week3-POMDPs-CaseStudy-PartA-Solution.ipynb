{"cells":[{"cell_type":"markdown","metadata":{"id":"96d9b0ee"},"source":["# 43008: Reinforcement Learning\n","\n","## Week 3 Part A: Partially Observable MDPs: Scenario-1\n","* POMDPs\n","\n","### What you will learn?\n","1. Create/Setup POMDPs from given case studie\n","2. Simulate the POMDP and estimate the rewards based on a policy"],"id":"96d9b0ee"},{"cell_type":"markdown","source":["\n","## **Scenario: Robot Navigation in a Noisy Environment**\n","\n","**Background:**  \n","Imagine a small robot tasked with navigating through a grid-like environment to reach a target location while avoiding obstacles. The robot's sensors provide noisy readings that can be either \"Clear\" or \"Obstacle,\" but due to sensor inaccuracies, the readings are not always reliable.\n","\n","The environment is represented as a grid with four locations: A, B, C, and D. The robot starts at location A and needs to reach location D to successfully complete its task. However, there is a possibility of encountering obstacles that might force the robot to change its path.\n","\n","* **States**: A, B, C, D (Locations in the grid)\n","* **Actions**: Up, Down, Left, Right (Robot movement directions)\n","* **Observations**: Clear (No obstacle detected), Obstacle (Obstacle detected)\n","* **Transition Probabilities**: The robot's movements are not perfectly deterministic, and there's a chance of unintended movement.\n","* **Observation Probabilities**: The robot's sensors have a chance of providing incorrect readings due to noise.\n","* **Rewards**: The robot receives rewards for reaching the goal, avoiding obstacles, and penalties for collisions or wrong movements.\n","* **Discount Factor**: Determines the trade-off between immediate and future rewards.\n","* **Objective**: The robot's goal is to navigate from location A to location D while avoiding obstacles and taking efficient actions to maximize its cumulative reward over time.\n","\n","\n","**Scenario Flow:**\n","\n","1. The robot starts at location A.\n","2. It selects an action (movement direction) based on its current state and observations from the sensors.\n","3. The robot receives immediate rewards based on the chosen action and its resulting state.\n","4. The robot's sensors provide an observation (Clear or Obstacle) about the current state, which might be noisy.\n","5. The robot updates its belief about the current state based on the observation and transition probabilities.\n","6. Steps 2 to 5 are repeated for each time step until the robot reaches location D or a specified number of time steps is reached.\n","7. The robot aims to choose actions that lead to a high cumulative reward, considering the uncertainties introduced by noisy sensors and probabilistic transitions.\n","\n","Throughout the scenario, the robot's optimal policy will help it make decisions that balance exploration, observation, and action to successfully navigate through the environment and reach the target location with maximum cumulative reward.\n","\n","\n","\n","**Scenario visualization**\n","```\n","A --- Right ---> B\n","|                |\n","|                |\n","Down             Down\n","|                |\n","|                |\n","V                V\n","D <--- Left ---  C\n","```\n","\n"],"metadata":{"id":"lGU39qDUviJ9"},"id":"lGU39qDUviJ9"},{"cell_type":"markdown","source":["#### 1. POMDP definition\n","\n","<img src='https://drive.google.com/uc?id=1e3agtMbgHflQe8YxGJATx9doaCU990To' height=350>"],"metadata":{"id":"BtyqDaQx19Ip"},"id":"BtyqDaQx19Ip"},{"cell_type":"code","source":["# Create a generic class for POMDP\n","import numpy as np\n","\n","class PartialObservationPOMDP:\n","    def __init__(self, states, actions, observations, transition_probs, observation_probs, rewards, discount_factor):\n","        self.states = states                       # S, set of states\n","        self.actions = actions                     # A, set of actions\n","        self.observations = observations           # O, set of observations\n","        self.transition_probs = transition_probs   # P, state transition probalities\n","        self.observation_probs = observation_probs # Z, observation probalities\n","        self.rewards = rewards                     # R, rewards\n","        self.discount_factor = discount_factor     # gamma, discount factor\n","\n","    # Create a function to simulate the POMDP, state transitions based on choosen action guided by the policy, and estimate the rewards\n","    def simulate(self, start_state, policy, max_steps):\n","        current_state = start_state\n","        total_reward = 0\n","\n","        print(f\"Starting at location {current_state}\")\n","\n","        # Iterate over a fixed number of step provide\n","        for step in range(max_steps):\n","\n","            # Select action based on the given policy\n","            action = policy[current_state]\n","\n","            # Find the next state based on the action taken\n","            next_state = np.random.choice(self.states, p=self.transition_probs[action][self.states.index(current_state)])\n","\n","            # Select an observation based on the next state\n","            observation = np.random.choice(self.observations, p=self.observation_probs[next_state])\n","\n","            # Find the reward based on the current_state, action selected, and next_state\n","            reward = self.rewards[self.states.index(current_state)][self.actions.index(action)][self.states.index(next_state)]\n","\n","            print(f\"Step {step + 1}: Action: {action}, Next Location: {next_state}, Observation: {observation}, Reward: {reward}\")\n","\n","            # Update the totoal reward and update the current state for next interation.\n","            total_reward += reward\n","            current_state = next_state\n","\n","            # Stop when the terminal state is reached.\n","            if current_state == \"D\":\n","                print(\"Reached destination D!\")\n","                break\n","\n","        # Display the total Rewards accumulated\n","        print(f\"Total Reward: {total_reward}\")\n"],"metadata":{"id":"XWcVK5JanU0P","executionInfo":{"status":"ok","timestamp":1724065455837,"user_tz":-600,"elapsed":365,"user":{"displayName":"Nabin Sharma","userId":"12891539814891469122"}}},"id":"XWcVK5JanU0P","execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Define the States, Actions, Observations, transition Probabilities, and rewards based on the given scenario"],"metadata":{"id":"adMBbmPp6-HZ"},"id":"adMBbmPp6-HZ"},{"cell_type":"code","source":["# Define states, actions, observations, transition probabilities, observation probabilities, rewards, and discount factor\n","states = [\"A\", \"B\", \"C\", \"D\"]\n","actions = [\"Up\", \"Down\", \"Left\", \"Right\"]\n","observations = [\"Clear\", \"Obstacle\"]\n","\n","transition_probs = {\n","    \"Up\": np.array([[0.1, 0.5, 0.3, 0.1],     # Given Action='Up', Transition Probability from  State='A' to State ={\"A\", \"B\", \"C\", \"D\"}\n","                    [0, 0, 1, 0],             # Given Action='Up', Transition Probability from  State='B' to State ={\"A\", \"B\", \"C\", \"D\"}\n","                    [0.1, 0.7, 0.1, 0.1],\n","                    [0.1, 0.5, 0.1, 0.3]]),\n","    \"Down\": np.array([[0, 0, 0, 1],   # Given Action='Down', Transition Probability from  State='A' to State ={\"A\", \"B\", \"C\", \"D\"}\n","                      [0, 0, 1, 0],\n","                      [0.1, 0.1, 0.7, 0.1],   # Given Action='Down', Transition Probability from  State='C' to State ={\"A\", \"B\", \"C\", \"D\"}\n","                      [0.1, 0.1, 0.1, 0.7]]),\n","    \"Left\": np.array([[0.7, 0.1, 0.1, 0.1],   # Given Action='Left', Transition Probability from  State='A' to State ={\"A\", \"B\", \"C\", \"D\"}\n","                      [0.7, 0.1, 0.1, 0.1],   # Given Action='Left', Transition Probability from  State='B' to State ={\"A\", \"B\", \"C\", \"D\"}\n","                      [0, 0, 0, 1],\n","                      [0.1, 0.1, 0.1, 0.7]]),\n","    \"Right\": np.array([[0, 1, 0, 0],  # Given Action='Right', Transition Probability from  State='A' to State ={\"A\", \"B\", \"C\", \"D\"}\n","                       [0.1, 0.7, 0.1, 0.1],\n","                       [0.1, 0.1, 0.7, 0.1],\n","                       [0.1, 0.3, 0.3, 0.3]]) # Given Action='Right', Transition Probability from  State='D' to State ={\"A\", \"B\", \"C\", \"D\"}\n","}\n","observation_probs = {\n","    \"A\": [0.8, 0.2], # Given State='A', Probability of observation=[\"Clear\", \"Obstacle\"] | High probability no obstacle: 0.8\n","    \"B\": [0.6, 0.4], # Given State='B', Probability of observation=[\"Clear\", \"Obstacle\"] | Probability obstacle is 0.4\n","    \"C\": [0.9, 0.1], # Given State='C', Probability of observation=[\"Clear\", \"Obstacle\"] | High probability no obstacle: 0.9\n","    \"D\": [0.7, 0.3]  # Given State='A', Probability of observation=[\"Clear\", \"Obstacle\"] | High probability no obstacle :0.7\n","}\n","rewards = np.array([\n","    # Current_State: A, [Action X State]\n","    [[0, 0, 0, 0], # Rewards for Action = 'Up', and Next_State ={'A', 'B', 'C', 'D'}\n","     [0, 0, 0, 10],   # Rewards for Action = 'Down', and Next_State ={'A', 'B', 'C', 'D'}\n","     [0, 0, 0, 0],   # Rewards for Action = 'Left', and Next_State ={'A', 'B', 'C', 'D'}\n","     [0, 5, 0, 0]], # Rewards for Action = 'Right', and Next_State ={'A', 'B', 'C', 'D'}\n","\n","    # Current_State: B, [Action X State]\n","    [[0, 0, 0, 0],   # Rewards for Action = 'Up', and Next_State ={'A', 'B', 'C', 'D'}\n","     [0, 0, 8, 0],   # Rewards for Action = 'Down', and Next_State ={'A', 'B', 'C', 'D'}\n","     [-5, 0, 0, 0],  # Rewards for Action = 'Left', and Next_State ={'A', 'B', 'C', 'D'}\n","     [0, 0, 0, 0]],  # Rewards for Action = 'Right', and Next_State ={'A', 'B', 'C', 'D'}\n","\n","    # Current_State: C, [Action X State]\n","    [[0, -5, 0, 0], # Rewards for Action = 'Up', and Next_State ={'A', 'B', 'C', 'D'}\n","     [0, 0, 0, 0],   # Rewards for Action = 'Down', and Next_State ={'A', 'B', 'C', 'D'}\n","     [0, 0, 0, 10],   # Rewards for Action = 'Left', and Next_State ={'A', 'B', 'C', 'D'}\n","     [0, 0, 0, 0]], # Rewards for Action = 'Right', and Next_State ={'A', 'B', 'C', 'D'}\n","\n","    # Current_State: D, [Action X State]\n","    [[-10, 0, 0, 0], # Rewards for Action = 'Up', and Next_State ={'A', 'B', 'C', 'D'}\n","     [0, 0, 0, 0],   # Rewards for Action = 'Down', and Next_State ={'A', 'B', 'C', 'D'}\n","     [0, 0, 0, 0],   # Rewards for Action = 'Left', and Next_State ={'A', 'B', 'C', 'D'}\n","     [0, 0, -10, 10]]  # Rewards for Action = 'Right', and Next_State ={'A', 'B', 'C', 'D'}\n","])\n","\n","discount_factor = 0.9\n","\n"],"metadata":{"id":"UZCmu8aQ6-3T","executionInfo":{"status":"ok","timestamp":1724065458097,"user_tz":-600,"elapsed":3,"user":{"displayName":"Nabin Sharma","userId":"12891539814891469122"}}},"id":"UZCmu8aQ6-3T","execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["#### 3. Test the POMDP by creating an object, defining a policy to use and simulate"],"metadata":{"id":"wrHXM3BB7CP-"},"id":"wrHXM3BB7CP-"},{"cell_type":"code","source":["# Create a POMDP instance\n","roboNavPOMDP = PartialObservationPOMDP(states, actions, observations, transition_probs, observation_probs, rewards, discount_factor)\n","\n","# Define a simple random policy (e.g., always go \"Right\")\n","roboPolicy1 = {\"A\": \"Right\", \"B\": \"Right\", \"C\": \"Right\", \"D\": \"Right\"}\n","\n","# Define a simple deterministic policy (e.g., always go \"Right\")\n","roboPolicy2 = {\"A\": \"Right\", \"B\": \"Down\", \"C\": \"Left\", \"D\": \"up\"}\n","\n","# Define a simple deterministic policy (e.g., always go \"Down\")\n","roboPolicy3 = {\"A\": \"Down\", \"B\": \"Down\", \"C\": \"Down\", \"D\": \"Down\"}\n","\n","# Simulate the POMDP with partial observation\n","max_steps = 100  # You can adjust this number\n","print(\"Simulation -1\")\n","roboNavPOMDP.simulate(start_state=\"A\", policy=roboPolicy1, max_steps=max_steps)\n","\n","print(\"\\n Simulation -2\")\n","roboNavPOMDP.simulate(start_state=\"A\", policy=roboPolicy2, max_steps=max_steps)\n","\n","print(\"\\n Simulation -3\")\n","roboNavPOMDP.simulate(start_state=\"A\", policy=roboPolicy3, max_steps=max_steps)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0i2VfH5H7Bi6","executionInfo":{"status":"ok","timestamp":1724065461331,"user_tz":-600,"elapsed":388,"user":{"displayName":"Nabin Sharma","userId":"12891539814891469122"}},"outputId":"713866cd-556b-4e6f-b107-5367509f307d"},"id":"0i2VfH5H7Bi6","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Simulation -1\n","Starting at location A\n","Step 1: Action: Right, Next Location: B, Observation: Clear, Reward: 5\n","Step 2: Action: Right, Next Location: B, Observation: Clear, Reward: 0\n","Step 3: Action: Right, Next Location: C, Observation: Clear, Reward: 0\n","Step 4: Action: Right, Next Location: C, Observation: Clear, Reward: 0\n","Step 5: Action: Right, Next Location: C, Observation: Clear, Reward: 0\n","Step 6: Action: Right, Next Location: C, Observation: Clear, Reward: 0\n","Step 7: Action: Right, Next Location: C, Observation: Clear, Reward: 0\n","Step 8: Action: Right, Next Location: C, Observation: Clear, Reward: 0\n","Step 9: Action: Right, Next Location: C, Observation: Clear, Reward: 0\n","Step 10: Action: Right, Next Location: C, Observation: Clear, Reward: 0\n","Step 11: Action: Right, Next Location: C, Observation: Clear, Reward: 0\n","Step 12: Action: Right, Next Location: C, Observation: Clear, Reward: 0\n","Step 13: Action: Right, Next Location: C, Observation: Clear, Reward: 0\n","Step 14: Action: Right, Next Location: C, Observation: Clear, Reward: 0\n","Step 15: Action: Right, Next Location: C, Observation: Clear, Reward: 0\n","Step 16: Action: Right, Next Location: C, Observation: Clear, Reward: 0\n","Step 17: Action: Right, Next Location: C, Observation: Obstacle, Reward: 0\n","Step 18: Action: Right, Next Location: C, Observation: Clear, Reward: 0\n","Step 19: Action: Right, Next Location: C, Observation: Clear, Reward: 0\n","Step 20: Action: Right, Next Location: C, Observation: Clear, Reward: 0\n","Step 21: Action: Right, Next Location: C, Observation: Clear, Reward: 0\n","Step 22: Action: Right, Next Location: B, Observation: Obstacle, Reward: 0\n","Step 23: Action: Right, Next Location: C, Observation: Clear, Reward: 0\n","Step 24: Action: Right, Next Location: D, Observation: Clear, Reward: 0\n","Reached destination D!\n","Total Reward: 5\n","\n"," Simulation -2\n","Starting at location A\n","Step 1: Action: Right, Next Location: B, Observation: Obstacle, Reward: 5\n","Step 2: Action: Down, Next Location: C, Observation: Clear, Reward: 8\n","Step 3: Action: Left, Next Location: D, Observation: Obstacle, Reward: 10\n","Reached destination D!\n","Total Reward: 23\n","\n"," Simulation -3\n","Starting at location A\n","Step 1: Action: Down, Next Location: D, Observation: Clear, Reward: 10\n","Reached destination D!\n","Total Reward: 10\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"o4XIA6CyDJSS"},"id":"o4XIA6CyDJSS","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}