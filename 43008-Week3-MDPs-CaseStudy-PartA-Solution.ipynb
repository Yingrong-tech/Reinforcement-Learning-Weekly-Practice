{"cells":[{"cell_type":"markdown","metadata":{"id":"96d9b0ee"},"source":["# 43008: Reinforcement Learning\n","\n","## Week 3 Part A: Markov Decision Process (MDPs):\n","* MDPs\n","\n","### What you will learn?\n","1. Create/Setup MDPs from different case studies."],"id":"96d9b0ee"},{"cell_type":"markdown","source":["\n","\n","**Scenario: Efficient Package Delivery with Drones in a City**\n","\n","**Background:**  \n","In today's fast-paced urban environments, drones have emerged as an innovative solution for delivering packages. They offer the advantage of swift aerial transport, bypassing the busy city streets. The city is divided into distinct zones: Warehouse (W), Location A, Location B, and Location C. Each of these zones serves as a potential delivery or pickup point. The drone's mission is simple: pick up packages from the Warehouse and ensure timely deliveries to the designated locations.\n","\n","The drone starts its journey from the Warehouse, with a package ready to be delivered to any of the three locations. The most efficient route is determined by various factors, such as air traffic, weather conditions, and distance to the destination. However, in this scenario, we are primarily focusing on choosing the best delivery route without considering the battery constraints.\n","\n","* **States**: The drone's current location (W, A, B, C).\n","* **Actions**: Fly to W/A/B/C.\n","* **Transition Probabilities**: The drone's path isn't deterministic due to changing city dynamics, but for the purpose of this model, we assume it always reaches its intended destination.\n","* **Rewards**: The drone receives rewards based on successful deliveries, with varying rewards for different locations. The reward structure is influenced by factors like distance, importance of the delivery, and potential penalties for delays.\n","* **Objective**: The drone's primary goal is to ensure that packages are delivered on time to the correct locations. It aims to maximize its total reward by choosing the most efficient delivery routes.\n","\n","**Scenario Flow:**\n","\n","1. The drone starts its journey from the Warehouse (W) with a package.\n","2. Based on its current location and the package's destination, it decides the next best location to fly to.\n","3. Upon reaching the next location, the drone completes the delivery and receives feedback in the form of rewards.\n","4. The drone continues to make decisions on the next best location to fly to, delivering packages and collecting rewards.\n","5. This process is repeated for each delivery until the drone completes all its deliveries or until a specified number of deliveries are reached.\n","6. The overarching goal is to make decisions that maximize the total rewards, ensuring that packages are delivered efficiently and on time.\n","\n","Throughout this scenario, the optimal delivery strategy will guide the drone in making the best decisions to ensure all packages are delivered promptly, maximizing the total reward.\n"],"metadata":{"id":"lGU39qDUviJ9"},"id":"lGU39qDUviJ9"},{"cell_type":"code","source":["import numpy as np\n","\n","class DroneDeliveryMDP:\n","    def __init__(self, states, actions, transition_matrix, reward_matrix, discount_factor=1.0):\n","        self.states = states\n","        self.actions = actions\n","        self.transition_matrix = transition_matrix\n","        self.reward_matrix = reward_matrix\n","        self.discount_factor = discount_factor\n","\n","    def simulate(self, start_state, policy, max_steps):\n","        current_state = start_state\n","        total_reward = 0\n","\n","        print(f\"Starting at location {current_state}\")\n","\n","        for step in range(max_steps):\n","            action = policy[current_state]\n","\n","            # Directly set the next state based on the action, without using np.random.choice\n","            action_destination = action.split(\"_\")[1]\n","            next_state = action_destination\n","\n","            reward = self.reward_matrix[self.states.index(current_state)][self.actions.index(action)]\n","\n","            print(f\"Step {step + 1}: Action: {action}, Next Location: {next_state}, Reward: {reward}\")\n","\n","            total_reward += reward\n","            current_state = next_state\n","\n","        print(f\"Total Reward: {total_reward}\")\n","\n","# Define states\n","states = ['W', 'A', 'B', 'C']\n","\n","# Define the actions\n","actions = [\"Fly_W\", \"Fly_A\", \"Fly_B\", \"Fly_C\"]\n","\n","# Transition matrix where rows represent current state, columns represent actions, and the inner lists represent the next state probabilities.\n","transition_matrix = [\n","    [0, 1, 0, 0],  # from 'W'\n","    [1, 0, 0, 0],  # from 'A'\n","    [0, 1, 0, 0],  # from 'B'\n","    [1, 0, 0, 0]   # from 'C'\n","]\n","\n","# Reward matrix where rows represent current state and columns represent actions.\n","reward_matrix = [\n","    [0, 10, -4, -6],  # from 'W'\n","    [-2, 0, 10, -3],  # from 'A'\n","    [-3, -1, 0, 10],  # from 'B'\n","    [10, -3, -2, 0]   # from 'C'\n","]\n","\n","# Create an MDP instance\n","mdp = DroneDeliveryMDP(states, actions, transition_matrix, reward_matrix)\n","\n","# Define a simple policy (e.g., always move to the next location alphabetically)\n","policy = {\"W\": \"Fly_A\", \"A\": \"Fly_B\", \"B\": \"Fly_C\", \"C\": \"Fly_W\"}\n","\n","# Simulate the MDP\n","max_steps = 10\n","mdp.simulate(start_state=\"W\", policy=policy, max_steps=max_steps)\n"],"metadata":{"id":"eoUOITyMcP1X","executionInfo":{"status":"ok","timestamp":1692854145439,"user_tz":-600,"elapsed":5,"user":{"displayName":"Gitarth Vaishnav","userId":"04717357691861531820"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9dd00a99-d292-4b05-ca30-86f559572b3d"},"id":"eoUOITyMcP1X","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting at location W\n","Step 1: Action: Fly_A, Next Location: A, Reward: 10\n","Step 2: Action: Fly_B, Next Location: B, Reward: 10\n","Step 3: Action: Fly_C, Next Location: C, Reward: 10\n","Step 4: Action: Fly_W, Next Location: W, Reward: 10\n","Step 5: Action: Fly_A, Next Location: A, Reward: 10\n","Step 6: Action: Fly_B, Next Location: B, Reward: 10\n","Step 7: Action: Fly_C, Next Location: C, Reward: 10\n","Step 8: Action: Fly_W, Next Location: W, Reward: 10\n","Step 9: Action: Fly_A, Next Location: A, Reward: 10\n","Step 10: Action: Fly_B, Next Location: B, Reward: 10\n","Total Reward: 100\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","class DroneDeliveryMDP:\n","    def __init__(self, states, actions, transition_probs, rewards, discount_factor=1.0):\n","        self.states = states\n","        self.actions = actions\n","        self.transition_probs = transition_probs\n","        self.rewards = rewards\n","        self.discount_factor = discount_factor\n","\n","    def simulate(self, start_state, policy, max_steps):\n","        current_state = start_state\n","        total_reward = 0\n","\n","        print(f\"Starting at location {current_state}\")\n","\n","        for step in range(max_steps):\n","            action = policy[current_state]\n","            next_state = np.random.choice(self.states, p=[self.transition_probs[current_state][action][s_prime] for s_prime in self.states])\n","            reward = self.rewards[current_state][action]\n","\n","            print(f\"Step {step + 1}: Action: {action}, Next Location: {next_state}, Reward: {reward}\")\n","\n","            total_reward += reward\n","            current_state = next_state\n","\n","        print(f\"Total Reward: {total_reward}\")\n","\n","# Define states\n","states = ['W', 'A', 'B', 'C']\n","\n","# Define the actions possible for each state. For each state, possible actions are moving to the other states.\n","actions = {s: [f\"Fly_{a}\" for a in states if a != s] for s in states}\n","\n","# Define the transition probabilities. Since it's deterministic, the transition is only possible to the destination state with probability 1.\n","transition_probs = {\n","    s: {f\"Fly_{a}\": {s_prime: 1 if a == s_prime else 0 for s_prime in states} for a in states if a != s}\n","    for s in states\n","}\n","\n","# Define the rewards for each transition. If there's no defined reward for a transition, it's assumed to be 0.\n","rewards = {\n","    s: {f\"Fly_{a}\": 0 for a in states if a != s} for s in states\n","}\n","\n","delivery_rewards = 10\n","\n","rewards['W'].update({\"Fly_A\": delivery_rewards, \"Fly_B\": -4, \"Fly_C\": -6})\n","rewards['A'].update({\"Fly_W\": -2, \"Fly_B\": delivery_rewards, \"Fly_C\": -3})\n","rewards['B'].update({\"Fly_W\": -3, \"Fly_A\": -1, \"Fly_C\": delivery_rewards})\n","rewards['C'].update({\"Fly_W\": delivery_rewards, \"Fly_A\": -3, \"Fly_B\": -2})\n","\n","# Create an MDP instance\n","mdp = DroneDeliveryMDP(states, actions, transition_probs, rewards)\n","\n","# Define a simple policy (e.g., always move to the next location alphabetically)\n","policy = {\"W\": \"Fly_A\", \"A\": \"Fly_B\", \"B\": \"Fly_C\", \"C\": \"Fly_W\"}\n","\n","# Simulate the MDP\n","max_steps = 10\n","mdp.simulate(start_state=\"W\", policy=policy, max_steps=max_steps)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XWcVK5JanU0P","executionInfo":{"status":"ok","timestamp":1692854145438,"user_tz":-600,"elapsed":624,"user":{"displayName":"Gitarth Vaishnav","userId":"04717357691861531820"}},"outputId":"fca665e4-283f-47a3-9b77-d0c5baf60bc3"},"id":"XWcVK5JanU0P","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting at location W\n","Step 1: Action: Fly_A, Next Location: A, Reward: 10\n","Step 2: Action: Fly_B, Next Location: B, Reward: 10\n","Step 3: Action: Fly_C, Next Location: C, Reward: 10\n","Step 4: Action: Fly_W, Next Location: W, Reward: 10\n","Step 5: Action: Fly_A, Next Location: A, Reward: 10\n","Step 6: Action: Fly_B, Next Location: B, Reward: 10\n","Step 7: Action: Fly_C, Next Location: C, Reward: 10\n","Step 8: Action: Fly_W, Next Location: W, Reward: 10\n","Step 9: Action: Fly_A, Next Location: A, Reward: 10\n","Step 10: Action: Fly_B, Next Location: B, Reward: 10\n","Total Reward: 100\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"THmgh_owkP1b"},"id":"THmgh_owkP1b","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}