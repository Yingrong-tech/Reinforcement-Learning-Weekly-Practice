{"cells":[{"cell_type":"markdown","metadata":{"id":"96d9b0ee"},"source":["# 43008: Reinforcement Learning\n","\n","## Week 4: Solving MDPs, Part-1:\n","* Policy Iteration Algorithm\n","\n","### What you will learn?\n","* Implement Policy Evaluation Algorithm\n","* Implement Policy Improvement Algorithm\n","* Implement Policy Iteration Algorithm\n","* MDP Implementation\n","* Solving a Case Study"],"id":"96d9b0ee"},{"cell_type":"markdown","source":["## Algorithms"],"metadata":{"id":"Zcpyn03Vhim0"},"id":"Zcpyn03Vhim0"},{"cell_type":"markdown","source":["### 1. Policy Iteration"],"metadata":{"id":"vLq7AnvPQM1E"},"id":"vLq7AnvPQM1E"},{"cell_type":"markdown","source":["\n","#### 1(a). Policy Evaluation\n","\n","Policy Evaluation is a process that estimates the state-value function $v_\\pi$ for a given policy $\\pi$ in a Markov Decision Process (MDP). It uses the Bellman expectation equation for policy evaluation:\n","\n","$$ v_\\pi(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma v_\\pi(s') \\right] $$\n","\n","However, for deterministic policies, where each state $s$ has a specific action $a$ defined by the policy $\\pi$, the equation simplifies to:\n","\n","$$ v_\\pi(s) = \\sum_{s', r} p(s', r | s, \\pi(s)) \\left[ r + \\gamma v_\\pi(s') \\right] $$\n","\n","Where:\n","- $v_\\pi(s)$ is the state-value of state $s$ under policy $\\pi$.\n","- $p(s', r | s, a)$ is the probability of transitioning to state $s'$ and receiving reward $r$ when action $a$ is taken in state $s$.\n","- $\\gamma$ is the discount factor.\n","\n","#### Algorithm (Ref and source: Sutton & Barto, Chapter 4)\n","\n","<img src='https://drive.google.com/uc?id=1w8IDWm9ouH9EOrwgfw1hyFgeDzlwdIoZ' height=300>\n"],"metadata":{"id":"I3bkYVJWgmCG"},"id":"I3bkYVJWgmCG"},{"cell_type":"markdown","source":["**Pseudo-code**:\n","```plain\n","Function PolicyEvaluation(policy, transition_probs, rewards, gamma, eps):\n","    Initialize V for each state to 0\n","    Repeat:\n","        new_V = copy of V\n","        For each state s:\n","            action = policy's action for state s\n","            state_value = 0\n","            For each possible next state s' and corresponding reward r:\n","                Calculate expected value for s' using transition probabilities and rewards\n","                Update state_value using the calculated expected value\n","            Update new_V for state s with state_value\n","        If max change between new_V and V < eps:\n","            Break\n","        V = new_V\n","    Return V\n","```"],"metadata":{"id":"yf1GplWRCCRg"},"id":"yf1GplWRCCRg"},{"cell_type":"code","source":["def policy_evaluation(policy, transition_probs, rewards, gamma, theta, states):\n","    \"\"\"\n","    Evaluate the value function of a given policy using the Bellman expectation equation.\n","\n","    Parameters:\n","    - policy: The policy to be evaluated.\n","    - transition_probs: Transition model of the environment (p(s', r | s, a)).\n","    - rewards: Reward function/matrix.\n","    - gamma: Discount factor for future rewards.\n","    - theta: Convergence threshold. Stops evaluation once value function changes are smaller than eps.\n","    - states: List of all states.\n","\n","    Returns:\n","    - V: Value function of the given policy (v_pi).\n","    \"\"\"\n","\n","    # Initialization: Start with an arbitrary value function\n","    V = ## WRITE YOUR CODE HERE ##\n","\n","    while True:\n","        new_V = ## WRITE YOUR CODE HERE ## Hint: Just create a copy of the V\n","\n","        # Update each state's value based on the Bellman expectation equation\n","        for state in states: # Select a state from list of all states\n","            # Given a policy, we directly get the action for the state\n","            action = ## WRITE YOUR CODE HERE ## # Following the given policy select the next action for the state\n","\n","            # Initialize the state's value to 0 and delta to 0 for this iteration\n","            delta =  ## WRITE YOUR CODE HERE ##\n","            state_value= ## WRITE YOUR CODE HERE ##\n","\n","            # Compute the expected value for the state using its policy action\n","            for next_state in states:\n","                # Find Transition Prob for Next State given current state and action\n","                transition_prob = ## WRITE YOUR CODE HERE ##\n","                # Find the reward to the action selected based on the policy for the current state\n","                reward = ## WRITE YOUR CODE HERE ##\n","                # Update the state value for the action given the policy\n","                # Find the state value which is (transition prob X {Reward + gamma X V of next state})\n","                state_value += ## WRITE YOUR CODE HERE ##\n","\n","            # Assign the computed state value to the new value function\n","            new_V[state] = ## WRITE YOUR CODE HERE ##\n","\n","            # Convergence Check\n","            delta = max(delta, abs(new_V[state] - V[state]))\n","        # Check if delta is less than theta\n","        if ## WRITE YOUR CODE HERE ##:\n","            break\n","\n","        # Update the value function for the next iteration with the New_V\n","        V = ## WRITE YOUR CODE HERE ##\n","\n","    return V\n"],"metadata":{"id":"18FZJFjnJOye"},"execution_count":null,"outputs":[],"id":"18FZJFjnJOye"},{"cell_type":"markdown","source":["#### 1(b). Policy Improvement\n","\n","Policy Improvement is a process used to refine or enhance an existing policy based on a given state-value function $V$. The objective is to make the policy \"greedy\" with respect to the current value function, ensuring that at each state, the action that maximizes the expected return, based on the current value function, is chosen.\n","\n","The core formula used to select the optimal action for a state $s$ is:\n","\n","$$ \\pi'(s) = \\arg\\max_a \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma V(s') \\right] $$\n","\n","Where:\n","- $\\pi'(s)$ is the improved policy's action for state $s$.\n","- $p(s', r | s, a)$ denotes the transition probability to state $s'$ and receiving reward $r$ when action $a$ is taken in state $s$.\n","- $\\gamma$ is the discount factor.\n","\n","**Pseudo-code**:\n","```plain\n","Function PolicyImprovement(V, transition_probs, rewards, actions, gamma):\n","    Initialize an empty policy new_policy\n","    For each state s:\n","        Initialize best_action to None and best_value to negative infinity\n","        For each action a available in state s:\n","            Initialize action_value to 0\n","            For each possible next state s' and corresponding reward r:\n","                Calculate expected value for s' based on transition probabilities, rewards, and V\n","                Update action_value with the calculated expected value\n","            If action_value is greater than best_value:\n","                Update best_value with action_value\n","                Update best_action with action a\n","        Update new_policy for state s with best_action\n","    Return new_policy\n","```\n"],"metadata":{"id":"oRYeZV0fgoVg"},"id":"oRYeZV0fgoVg"},{"cell_type":"code","source":["def policy_improvement(V, transition_probs, rewards, actions, gamma, states):\n","    \"\"\"\n","    Improve the policy based on a given value function using the Bellman optimality equation.\n","\n","    Parameters:\n","    - V: Current estimate of the value function.\n","    - transition_probs: Transition model of the environment.\n","    - rewards: Reward function.\n","    - actions: Dictionary of available actions for each state.\n","    - gamma: Discount factor for future rewards.\n","    - states: List of all states.\n","\n","    Returns:\n","    - new_policy: Improved policy.\n","    \"\"\"\n","\n","    new_policy = {}\n","\n","    # Iterate through each state to update its policy action\n","    for state in states:\n","\n","        # Initialize the best action and its corresponding value for this state\n","        best_action = ## WRITE YOUR CODE HERE ## Hint: None\n","        best_value = ## WRITE YOUR CODE HERE ## Hint: float('-inf')\n","\n","        # Evaluate each possible action to find the best one for this state\n","        for action in actions[state]:\n","            # Initialize the action value to zero\n","            action_value = ## WRITE YOUR CODE HERE ##\n","\n","            # Calculate the expected value of this action over all next possible states\n","            for next_state in states:\n","                # Find Transition probability for moving to next state, based on the current state and action\n","                transition_prob = ## WRITE YOUR CODE HERE ##\n","                # Reward for taking an action from the current state\n","                reward = ## WRITE YOUR CODE HERE ##\n","                # Updated the action value which is (Transition Prob * {Reward + gamma* V of next state}))\n","                action_value +=  ## WRITE YOUR CODE HERE ##\n","\n","            # Update best action if this action's value is higher\n","            if action_value > best_value:\n","                best_value = ## WRITE YOUR CODE HERE ## Hint: the computed action_value\n","                best_action = ## WRITE YOUR CODE HERE ## Hint: the corresponding action\n","\n","        # Assign the best action to the policy for this state\n","        new_policy[state] = ## WRITE YOUR CODE HERE ## Hint: The best_action\n","\n","    return new_policy\n"],"metadata":{"id":"rwhxA-PKJMyH"},"execution_count":null,"outputs":[],"id":"rwhxA-PKJMyH"},{"cell_type":"markdown","source":["#### 1(c). Policy Iteration Algorithm\n","\n","Policy Iteration is a two-step process used to determine the optimal policy and the associated state-value function in a Markov Decision Process (MDP). The algorithm alternates between **Policy Evaluation** and **Policy Improvement** until the policy converges to the optimal policy.\n","\n","1. **Policy Evaluation**: Compute the state-value function $v_\\pi$ for the current policy $\\pi$ using the Bellman expectation equation:\n","\n","$$ v_\\pi(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma v_\\pi(s') \\right] $$\n","\n","2. **Policy Improvement**: Update the policy by making it \"greedy\" with respect to the current state-value function:\n","\n","$$ \\pi'(s) = \\arg\\max_a \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma v_\\pi(s') \\right] $$\n","\n","The process is repeated until the policy remains unchanged after an improvement step, indicating convergence to the optimal policy.\n","\n","**Pseudo-code**:\n","```plain\n","Function PolicyIteration(states, actions, transition_probs, rewards, gamma, eps):\n","    Initialize an arbitrary policy (e.g., the first available action for each state)\n","    Repeat:\n","        Evaluate the current policy to get the state-value function V\n","        Improve the policy based on V to get new_policy\n","        If new_policy is the same as the current policy:\n","            Break\n","        Update the current policy to new_policy\n","    Return the final state-value function V and the optimal policy\n","```"],"metadata":{"id":"lvJR5phggqIl"},"id":"lvJR5phggqIl"},{"cell_type":"code","source":["def policy_iteration(states, actions, transition_probs, rewards, gamma=0.9, theta=1e-3, debug_print=False):\n","    \"\"\"\n","    Implements the policy iteration algorithm to find the optimal policy and value function.\n","\n","    Parameters:\n","    - states: List of states.\n","    - actions: Dictionary of available actions for each state.\n","    - transition_probs: Transition model of the environment.\n","    - rewards: Reward dictionary.\n","    - gamma: Discount factor for future rewards.\n","    - theta: Convergence threshold for policy evaluation phase.\n","\n","    Returns:\n","    - V: Optimal value function.\n","    - policy: Optimal policy.\n","    \"\"\"\n","\n","    # Initialization: Arbitrarily choose an initial policy\n","    # Hint: Here, we initialize the policy for each state as the first available action.\n","    policy = ## WRITE YOUR CODE HERE ##\n","    iteration_count = 0\n","\n","    while True:\n","        iteration_count += 1\n","\n","        # Policy Evaluation: Compute the state-value function for the current policy\n","        V =  ## WRITE YOUR CODE HERE ## call the Policy Evaluation function with the required parameters\n","\n","        if debug_print:\n","            print(f\"Policy Iteration {iteration_count}: Value Function after Policy Evaluation: {V}\")\n","\n","        # Policy Improvement: Update the policy to be greedy with respect to the current value function\n","        new_policy = ## WRITE YOUR CODE HERE ## call the Policy Improvement function with the required parameters\n","\n","        if debug_print:\n","            print(f\"Policy Iteration {iteration_count}: Policy after Improvement: {new_policy} \\n\")\n","\n","        # Convergence Check: If the policy remains unchanged after the improvement step, it's optimal\n","        if new_policy == policy:\n","            break\n","\n","        # Update the policy for the next iteration\n","        policy = ## WRITE YOUR CODE HERE ## Hint: new policy\n","\n","    return V, policy\n"],"metadata":{"id":"_5gYuau0Ed6S"},"execution_count":null,"outputs":[],"id":"_5gYuau0Ed6S"},{"cell_type":"markdown","source":["## 2. MDP Implementation\n"],"metadata":{"id":"aEsILgcIgOYB"},"id":"aEsILgcIgOYB"},{"cell_type":"markdown","source":["This time we generalise the MDP structure with solving functions, so that we can use any given scenerio. There are possibilities that depending on the scenerio, we modify the functions."],"metadata":{"id":"3ekk_aJdguzk"},"id":"3ekk_aJdguzk"},{"cell_type":"code","source":["import numpy as np\n","import time\n","\n","class MDP:\n","    def __init__(self, states, actions, transition_matrix, reward_matrix, discount_factor=1.0):\n","        \"\"\"\n","        Initialize the MDP with given states, actions, transition probabilities, rewards, and discount factor.\n","\n","        Parameters:\n","        - states: List of states in the MDP\n","        - actions: List of actions available in the MDP\n","        - transition_matrix: Matrix where each row represents the current state, each column represents an action,\n","                             and the inner lists represent the next state probabilities.\n","        - reward_matrix: Matrix where each row represents the current state and each column represents an action.\n","        - discount_factor: Discount factor for future rewards (gamma in Sutton & Barto)\n","        \"\"\"\n","        self.states = ## WRITE YOUR CODE HERE ##\n","        self.actions = ## WRITE YOUR CODE HERE ##\n","        self.transition_matrix = ## WRITE YOUR CODE HERE ##\n","        self.reward_matrix = ## WRITE YOUR CODE HERE ##\n","        self.discount_factor = ## WRITE YOUR CODE HERE ##\n","\n","    # Converting the Transition Prob, rewards and actions to Dictionary.\n","    def convert_to_dictionary(self):\n","        \"\"\"\n","        Convert transition matrix and reward matrix to a dictionary format which is more intuitive for certain operations.\n","\n","        Returns:\n","        - transition_probs: Dictionary of transition probabilities\n","        - rewards: Dictionary of rewards for state-action pairs\n","        - actions: Dictionary of available actions for each state\n","        \"\"\"\n","        # Convert actions list to dictionary format\n","        actions = {state: [act for act in self.actions] for state in self.states}\n","\n","        # Initialize the transition_probs and rewards dictionaries\n","        transition_probs = {s: {} for s in self.states}\n","        rewards = {s: {} for s in self.states}\n","\n","        for i, s in enumerate(self.states):\n","            for j, a in enumerate(self.actions):\n","                transition_probs[s][a] = {}\n","                for k, s_prime in enumerate(self.states):\n","                    # Set the transition probability for s' from the matrix\n","                    transition_probs[s][a][s_prime] = self.transition_matrix[i][k]\n","\n","                    # Set the reward for action a in state s from the matrix\n","                    rewards[s][a] = self.reward_matrix[i][j]\n","\n","        return transition_probs, rewards, actions"],"metadata":{"id":"B7klopSFFB0T"},"id":"B7klopSFFB0T","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","##**3. Scenario: Efficient Package Delivery with Drones in a City**\n","\n","**Background:**  \n","In today's fast-paced urban environments, drones have emerged as an innovative solution for delivering packages. They offer the advantage of swift aerial transport, bypassing the busy city streets. The city is divided into distinct zones: Warehouse (W), Location A, Location B, and Location C. Each of these zones serves as a potential delivery or pickup point. The drone's mission is simple: pick up packages from the Warehouse and ensure timely deliveries to the designated locations.\n","\n","The drone starts its journey from the Warehouse, with a package ready to be delivered to any of the three locations. The most efficient route is determined by various factors, such as air traffic, weather conditions, and distance to the destination. However, in this scenario, we are primarily focusing on choosing the best delivery route without considering the battery constraints.\n","\n","* **States**: The drone's current location (W, A, B, C).\n","* **Actions**: Fly to W/A/B/C.\n","* **Transition Matrix**: The drone's path isn't deterministic due to changing city dynamics, but for the purpose of this model, we assume it always reaches its intended destination.\n","* **Rewards**: The drone receives rewards based on successful deliveries, with varying rewards for different locations. The reward structure is influenced by factors like distance, importance of the delivery, and potential penalties for delays.\n","* **Objective**: The drone's primary goal is to ensure that packages are delivered on time to the correct locations. It aims to maximize its total reward by choosing the most efficient delivery routes.\n","\n","**Scenario Flow:**\n","\n","1. The drone starts its journey from the Warehouse (W) with a package.\n","2. Based on its current location and the package's destination, it decides the next best location to fly to.\n","3. Upon reaching the next location, the drone completes the delivery and receives feedback in the form of rewards.\n","4. The drone continues to make decisions on the next best location to fly to, delivering packages and collecting rewards.\n","5. This process is repeated for each delivery until the drone completes all its deliveries or until a specified number of deliveries are reached.\n","6. The overarching goal is to make decisions that maximize the total rewards, ensuring that packages are delivered efficiently and on time.\n","\n","Throughout this scenario, the optimal delivery strategy will guide the drone in making the best decisions to ensure all packages are delivered promptly, maximizing the total reward.\n"],"metadata":{"id":"lGU39qDUviJ9"},"id":"lGU39qDUviJ9"},{"cell_type":"markdown","source":["### Define Specifics\n","* States\n","* Actions\n","* Transition Probabilities (Matrix)\n","* Rewards (Matrix)\n","\n","**Hint: Similar to what was done in Week-3 MDP Case Study**"],"metadata":{"id":"LQ1YeRabg_lx"},"id":"LQ1YeRabg_lx"},{"cell_type":"code","source":["# Define states\n","states = ## WRITE YOUR CODE HERE ## Hint: State are {W, A, B, C}\n","\n","# Define the actions\n","actions = ## WRITE YOUR CODE HERE ## Hint: Actions are {Fly_W, Fly_A, Fly_B, Fly_C}\n","\n","# Transition matrix where rows represent current state, columns represent actions, and the inner lists represent the next state probabilities.\n","transition_matrix = [\n","    [0, 1, 0, 0],  # from 'W'\n","    [0, 0, 1, 0],  # from 'A'\n","    [0, 0, 0, 1],  # from 'B'\n","    [1, 0, 0, 0]   # from 'C'\n","]\n","\n","# Reward matrix where rows represent current state and columns represent actions.\n","reward_matrix = [\n","    [0, 10, -4, -6],  # from 'W'\n","    [-2, 0, 10, -3],  # from 'A'\n","    [-3, -1, 0, 10],  # from 'B'\n","    [10, -3, -2, 0]   # from 'C'\n","]\n","\n"],"metadata":{"id":"tbEDVEEsam5V"},"id":"tbEDVEEsam5V","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create an instance of the MDP with the defined specifics\n","* States\n","* Actions\n","* Transition Probabilities (Matrix)\n","* Rewards (Matrix)"],"metadata":{"id":"CWN8TvZ6h2Mo"},"id":"CWN8TvZ6h2Mo"},{"cell_type":"code","source":["# Create an MDP instance\n","drone_delivery_MDP = ## WRITE YOUR CODE HERE ##"],"metadata":{"id":"CSoEEw1PhxYw"},"id":"CSoEEw1PhxYw","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert to Dictionary before proceeding with Policy Iteration\n","transition_probs, rewards, actions = ## WRITE YOUR CODE HERE ##"],"metadata":{"id":"p16AY3DnH91B"},"id":"p16AY3DnH91B","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Solve MDP using Policy Iteration:"],"metadata":{"id":"QZ__BZtgislG"},"id":"QZ__BZtgislG"},{"cell_type":"code","source":["# Execute Policy Iteration Algorithm\n","V, policy = policy_iteration(drone_delivery_MDP.states, actions, transition_probs, rewards, debug_print=True)\n","\n","# Display outcomes\n","print(\"\\n Policy Iteration: Value function: \", V)\n","print(\"Policy Iteration: Policy: \", policy)"],"metadata":{"id":"aBs5FGmginQF"},"id":"aBs5FGmginQF","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ze004Lebshhx"},"id":"Ze004Lebshhx","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}