{"cells":[{"cell_type":"markdown","metadata":{"id":"96d9b0ee"},"source":["# 43008: Reinforcement Learning\n","\n","## Week 6: Monte Carlo Methods - Prediction:\n","* First-Visit MC Algorithm for Policy Evaluation\n","* Every-Visit MC Algorithm for Policy Evaluation\n","\n","### What you will learn?\n","* Implement First Visit MC Algorithm for Gym based environments\n","* Implement Every Visit MC Algorithm for Gym based environmenmts"]},{"cell_type":"markdown","metadata":{"id":"l3WS7iE40G0K"},"source":["### Let's install some important helper packages and libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CzxML8yh2p9J"},"outputs":[],"source":["!pip install gym pyvirtualdisplay\n","!apt-get install -y xvfb ffmpeg"]},{"cell_type":"markdown","metadata":{"id":"CNchkeOL4MEy"},"source":["### Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fYuswECt2pS7"},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","from collections import defaultdict\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","from pyvirtualdisplay import Display\n","\n","from matplotlib import pyplot\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","from collections import defaultdict\n","from functools import partial\n","%matplotlib inline\n","plt.style.use('ggplot')"]},{"cell_type":"markdown","metadata":{"id":"Pk6GSJl-QRLN"},"source":["### Helper function for plotting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A35XgK3zME5R"},"outputs":[],"source":["def plot_blackjack(V, ax1, ax2):\n","    #Define Ranges for each aspects like Player_sum, Dealer_Show etc.\n","    player_sum = np.arange(12, 21 + 1)\n","    dealer_show = np.arange(1, 10 + 1)\n","    usable_ace = np.array([False, True])\n","    state_values = np.zeros((len(player_sum), len(dealer_show), len(usable_ace)))\n","\n","    #Extract State-value from V in a specific order/format for visualization\n","    for i, player in enumerate(player_sum):\n","        for j, dealer in enumerate(dealer_show):\n","            for k, ace in enumerate(usable_ace):\n","                state_values[i, j, k] = V[player, dealer, ace]\n","\n","    X, Y = np.meshgrid(player_sum, dealer_show)\n","\n","    ax1.plot_wireframe(X, Y, state_values[:, :, 0])\n","    ax2.plot_wireframe(X, Y, state_values[:, :, 1])\n","\n","    # Axes settings\n","    for ax in ax1, ax2:\n","        ax.set_zlim(-1, 1)\n","        ax.set_ylabel('Player Sum')\n","        ax.set_xlabel('Dealer Showing')\n","        ax.set_zlabel('State-Value')\n"]},{"cell_type":"markdown","metadata":{"id":"4i627BS34fru"},"source":["### Helper functions"]},{"cell_type":"markdown","metadata":{"id":"_eoSnBq_O2Ec"},"source":["#### Generate an episode using a policy"]},{"cell_type":"code","source":["def sample_policy(observation):\n","    score, dealer_score, usable_ace = observation\n","    return 0 if score >= 20 else 1"],"metadata":{"id":"oKd3R9e5YRJ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KrIy05DkIHhW"},"source":["#### Initialize the BlackJack Gym environment\n","\n","**Reference**: https://www.gymlibrary.dev/environments/toy_text/blackjack/"]},{"cell_type":"code","source":["## WRITE YOUR CODE HERE ##\n","env = # Hint: Check the reference link provided"],"metadata":{"id":"wEHA6GL8a9oR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Function to generate random episodes"],"metadata":{"id":"Q3ojylSdIaRx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fMOHNgGu56r2"},"outputs":[],"source":["def generate_episode(env, given_policy):\n","    \"\"\"Generate an episode using a given policy and the envirnment\"\"\"\n","\n","    # Initialize an empty list to store the episode's state, action, and reward tuples.\n","    states, actions, rewards = [], [], []\n","\n","    # Reset the environment to get the initial state.\n","    current_state = ## WRITE YOUR CODE HERE ##\n","\n","    # Initialize a flag to check if the episode is finished.\n","    done = ## WRITE YOUR CODE HERE ##\n","\n","    # Continue until the episode is done/terminating state not reached.\n","    while not done:\n","\n","        states.append(current_state)\n","\n","        # Get the action for the current state from the policy.\n","        action = ## WRITE YOUR CODE HERE ##\n","        ## WRITE YOUR CODE HERE ## Append action to the actions list\n","\n","\n","        # Take the chosen action in the environment.\n","        # Get the resulting next state, reward, whether the episode is done, and other info.\n","        next_state_tuple = ## WRITE YOUR CODE HERE ##\n","        next_state, reward, terminated, truncated, info = ## WRITE YOUR CODE HERE ##\n","        done = terminated or truncated\n","\n","        print(f\"  Action: {action}, Next state: {next_state}, Reward: {reward}, Done: {done}, Info: {info}\")\n","\n","        ## WRITE YOUR CODE HERE ## ## Append the rewards to the Reward list\n","        rewards.\n","\n","        # Update the current state to the next state.\n","        current_state = ## WRITE YOUR CODE HERE ##\n","    # Return the generated episode.\n","\n","    return states, actions, rewards\n"]},{"cell_type":"markdown","metadata":{"id":"Zcpyn03Vhim0"},"source":["## Monte-Carlo First-Visit Algorithm"]},{"cell_type":"markdown","metadata":{"id":"36iyL0tgjZ9H"},"source":["```plaintext\n","Algorithm: Monte Carlo Prediction\n","Input: π (policy to be evaluated)\n","Initialize:\n","    Q(s, a) arbitrarily, for all s ∈ S, a ∈ A(s)\n","    Returns(s, a) ← empty list, for all s ∈ S, a ∈ A(s)\n","Repeat forever (for each episode):\n","    Generate an episode following π: S0, A0, R1, ..., ST-1, AT-1, RT\n","    G ← 0\n","    For t = T-1 down to 0:\n","        G ← γG + Rt+1\n","        If the pair St, At appears in S0, A0, ..., St-1, At-1 for \"first-visit\"\n","        OR\n","        Always for \"every-visit\":\n","            Append G to Returns(St, At)\n","            Q(St, At) ← average(Returns(St, At))\n","```\n","\n","In the algorithm above:\n","- The distinction between first-visit and every-visit is made in the conditional statement.\n","- For first-visit, the state-action pair should not have been visited in the episode before time \\( t \\).\n","- For every-visit, the state-action pair is updated every time it's encountered."]},{"cell_type":"markdown","metadata":{"id":"Q_g4btwqjzvd"},"source":["<img src='https://drive.google.com/uc?id=18EN5SYHlr0yvTcFYKxIRWrIJsglpbsXn' height=300>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"so3ost9YeJBY"},"source":["### 1. First Visit MC Prediction\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U_UT1tzeWjgZ"},"outputs":[],"source":["# First Visit Monte Carlo Prediction/Policy Evaluation\n","def first_visit_mc_prediction(given_policy, env, n_episodes, gamma=1):\n","\n","    # First, we initialize the empty value table as a dictionary for storing the values of each state\n","    V = ## WRITE YOUR CODE HERE ##\n","    N = ## WRITE YOUR CODE HERE ##\n","\n","\n","    for nE in range(n_episodes):\n","\n","        # Next, we generate the epsiode and store the states and rewards\n","\n","        states, _, rewards = ## WRITE YOUR CODE HERE ##\n","        G = ## WRITE YOUR CODE HERE ##\n","\n","        # Next for each step, store the rewards to a variable R and states to S, and calculate\n","        # returns as a sum of rewards\n","\n","        for stepT in range(len(states) - 1, -1, -1): # Syntax: Len(Start, stop, step)\n","            R = ## WRITE YOUR CODE HERE ##\n","            S = ## WRITE YOUR CODE HERE ##\n","\n","            # Perform first visit MC, check if the state in the current episode is visited for the first time, if yes,\n","            # Increment the counter and find the rewards from that state onwards and assign it to the State-Value\n","\n","            if S not in states[:stepT]:\n","                ## Increment the First Visit Counter..  WRITE YOUR CODE HERE ##\n","\n","                for rIdx in range(len(rewards[stepT:])):\n","                  G += ## WRITE YOUR CODE HERE ## ##\n","\n","                V[S] += ## WRITE YOUR CODE HERE ##\n","\n","    # Return the State-Value\n","    return V\n"]},{"cell_type":"code","source":["# Excecute the First Visit MC Algorithm on the BlackJack Environment and find the State-Value over multiple episodes\n","value_first = ## WRITE YOUR CODE HERE ##"],"metadata":{"id":"jOtlTNFHILJc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Just for checking\n","for i in range(10):\n","  print(value_first.popitem())"],"metadata":{"id":"lNfanHlmcL6s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the Values and correlate with the Policy Visually\n","fig, axes = pyplot.subplots(nrows=2, figsize=(8, 16),\n","subplot_kw={'projection': '3d'})\n","axes[0].set_title('value function without usable ace')\n","axes[1].set_title('value function with usable ace')\n","plot_blackjack(value_first, axes[0], axes[1])"],"metadata":{"id":"YP6Dpgi2cYyL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R597AlgpXf0C"},"source":["### 2. Every Visit MC Prediction\n","\n","\n","<img src='https://drive.google.com/uc?id=1hKppuG-mSY32zW8xJN96DdMhuYk5764n' height=330>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QEEW8yxfdiz"},"outputs":[],"source":["# Every-Visit Monte Carlo\n","def every_visit_mc_prediction(given_policy, env, n_episodes, gamma=1):\n","\n","    # First, we initialize the empty value table as a dictionary for storing the values of each state\n","    V = ## WRITE YOUR CODE HERE ##\n","    N = ## WRITE YOUR CODE HERE ##\n","\n","    # Run through all episodes\n","    for nE in range(n_episodes):\n","\n","        # Next, we generate the epsiode and store the states and rewards\n","        states, _, rewards = ## WRITE YOUR CODE HERE ##\n","        G = ## WRITE YOUR CODE HERE ##\n","\n","        # Next for each step, store the rewards to a variable R and states to S, and calculate\n","        # returns as a sum of rewards\n","        for stepT in range(len(states) - 1, -1, -1): # Syntax: Len(Start, stop, step)\n","            ## WRITE YOUR CODE HERE ## For R and G\n","\n","            # Perform every visit MC, check if the state in the current episode is visited\n","            # Increment the counter for each visit and find the rewards from that state onwards and assign it to the State-Value\n","\n","            N[S] += ## WRITE YOUR CODE HERE ##\n","\n","            G += ## WRITE YOUR CODE HERE ##\n","\n","            V[S] += (## WRITE YOUR CODE HERE ##\n","\n","    return V\n"]},{"cell_type":"code","source":["# Excecute the Every Visit MC Algorithm on the BlackJack Environment and find the State-Value over multiple episodes\n","value_every = ## WRITE YOUR CODE HERE ##"],"metadata":{"id":"zdLR_oW6gbJD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Just For Checking\n","for i in range(10):\n","  print(value_every.popitem())"],"metadata":{"id":"rDTuoUfVgjio"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the Values and correlate with the Policy Visually\n","fig, axes = pyplot.subplots(nrows=2, figsize=(8, 16),\n","subplot_kw={'projection': '3d'})\n","axes[0].set_title('value function without usable ace')\n","axes[1].set_title('value function with usable ace')\n","plot_blackjack(value_every, axes[0], axes[1])"],"metadata":{"id":"S631E1bUh55R"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}